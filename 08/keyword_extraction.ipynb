{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "punctuation += '«»—…“”*№–'\n",
    "stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = './data'\n",
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA) if file.endswith('.zip')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pd.read_json(file, lines=True) for file in files], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17266, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.cache = {}\n",
    "    \n",
    "    def lemmatize(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        norm = self.morph.parse(token)[0]\n",
    "        norm = (norm.normal_form, norm.tag.POS)\n",
    "        self.cache[token] = norm\n",
    "        return norm\n",
    "\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> List[str]:\n",
    "    output = []\n",
    "    for token in word_tokenize(text):\n",
    "        token = token.strip(punctuation)\n",
    "        if not token:\n",
    "            continue\n",
    "            \n",
    "        lemma, pos_tag = lemmatizer.lemmatize(token)\n",
    "        if lemma in stopwords or pos_tag != 'NOUN':\n",
    "            continue\n",
    "            \n",
    "        output.append(lemma)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 3s, sys: 232 ms, total: 5min 4s\n",
      "Wall time: 5min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['title_norm'] = data['title'].apply(normalize)\n",
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[будущее, россия, конвертоплан]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[россия, чемпионат, мир, футболист, зенит, краневиттер, месси, жизнь, петербург]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[россия, запад, автоматизация, управление, оборона]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[открытие, богатство, месторождение, газа, средиземноморье, конфликт, ближний, восток]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[дорога, украина, принятие, закон, реинтеграция, донбасс]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[переворот, сознание, черногория, россия, попытка, смена, власть, страна]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[топливо, украина, контроль, система, европеец]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[фронт, кампания, марин, пена, власть]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[оружие, цска, плей-офф, лига, чемпион]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[новичок, багаж, великобритания, версия, отравление]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               title_norm\n",
       "0                                                         [будущее, россия, конвертоплан]\n",
       "1        [россия, чемпионат, мир, футболист, зенит, краневиттер, месси, жизнь, петербург]\n",
       "2                                     [россия, запад, автоматизация, управление, оборона]\n",
       "3  [открытие, богатство, месторождение, газа, средиземноморье, конфликт, ближний, восток]\n",
       "4                               [дорога, украина, принятие, закон, реинтеграция, донбасс]\n",
       "5               [переворот, сознание, черногория, россия, попытка, смена, власть, страна]\n",
       "6                                         [топливо, украина, контроль, система, европеец]\n",
       "7                                                  [фронт, кампания, марин, пена, власть]\n",
       "8                                                 [оружие, цска, плей-офф, лига, чемпион]\n",
       "9                                    [новичок, багаж, великобритания, версия, отравление]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['title_norm']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "We will use the same exact metric computation function that was used in class — for comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp+fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp+fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    \n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = lambda x: x\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=5, preprocessor=dummy, tokenizer=dummy)\n",
    "tfidf_matrix = tfidf.fit_transform(data['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i: word for i, word in enumerate(tfidf.get_feature_names())}\n",
    "keywords = [[id2word[w] for w in row.toarray()[0, :].argsort()[-10:]] for row in tfidf_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.09\n",
      "Recall -  0.12\n",
      "F1 -  0.09\n",
      "Jaccard -  0.05\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 1-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.1\n",
      "Recall -  0.13\n",
      "F1 -  0.1\n",
      "Jaccard -  0.06\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, preprocessor=dummy, tokenizer=dummy)\n",
    "tfidf_matrix = tfidf.fit_transform(data['content_norm'])\n",
    "\n",
    "id2word = {i: word for i, word in enumerate(tfidf.get_feature_names())}\n",
    "keywords = [[id2word[w] for w in row.toarray()[0, :].argsort()[-10:]] for row in tfidf_matrix]\n",
    "\n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to improve the metrics by only using 1-grams. Let's stick to this from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 1-grams on Title + Content\n",
    "Let's try applying TF-IDF to a concatenation of the title and the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.1\n",
      "Recall -  0.13\n",
      "F1 -  0.11\n",
      "Jaccard -  0.06\n"
     ]
    }
   ],
   "source": [
    "data['all_norm'] = data.apply(lambda x: x.title_norm + x.content_norm, axis=1)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=5, preprocessor=dummy, tokenizer=dummy)\n",
    "tfidf_matrix = tfidf.fit_transform(data['all_norm'])\n",
    "\n",
    "id2word = {i: word for i, word in enumerate(tfidf.get_feature_names())}\n",
    "keywords = [[id2word[w] for w in row.toarray()[0, :].argsort()[-10:]] for row in tfidf_matrix]\n",
    "\n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did get one extra point in F1. Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VoteRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(text: List[str], window_size: int = 5):\n",
    "    vocab = set(text)\n",
    "    word2id = {w: i for i, w in enumerate(vocab)}\n",
    "    id2word = {i: w for i, w in enumerate(vocab)}\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i: i+window_size]\n",
    "        for j, k in combinations(window, 2):\n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    return m, id2word\n",
    "\n",
    "\n",
    "def voterank(text: List[str], window_size: int = 5, topn: int = 5):\n",
    "    matrix, id2word = build_matrix(text, window_size)\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "    node2measure = nx.voterank(G)\n",
    "    \n",
    "    return [id2word[index] for index in node2measure[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 46min 54s, sys: 76 ms, total: 1h 46min 54s\n",
      "Wall time: 1h 46min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "keywords_nx = data['all_norm'].apply(lambda x: voterank(x, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.1\n",
      "Recall -  0.13\n",
      "F1 -  0.1\n",
      "Jaccard -  0.06\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using graph algorithms, we were able to beat the baseline, but not the previous best result. It also took a gigantic amount of time to compute everything, so playing with hyperparameters, although potentially useful, is not feasible in reasonable amounts of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
