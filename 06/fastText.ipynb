{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydtKaWvuwHx7"
   },
   "source": [
    "# Training fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wC2JCaZpdlnV"
   },
   "source": [
    "## Corpus parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oV7AlZP9dJ6C"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from tqdm import tqdm\n",
    "# from time import sleep\n",
    "# from lxml import etree, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5y5a6J_ndn2e"
   },
   "outputs": [],
   "source": [
    "# links = []\n",
    "\n",
    "# for topic in [33, 37, 10, 13, 36, 12, 34, 39]:\n",
    "#     for letter in 'абвгдезиклмнпростуфхцшщчуэюя':\n",
    "#         for page_num in range(30):\n",
    "#             r = requests.get(f'https://www.krugosvet.ru/taxonomy/term/{topic}/{letter}?page={page_num}')\n",
    "#             if r.status_code != 200:\n",
    "#                 continue\n",
    "#             page = html.fromstring(r.text)\n",
    "#             hrefs = page.xpath(\"//div[@class='article-teaser']/a/@href\")\n",
    "#             links += ['https://www.krugosvet.ru'+href for href in hrefs]\n",
    "            \n",
    "#     print(f'After topic: {topic}, links collected: {len(links)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87x27UGZeEoA"
   },
   "outputs": [],
   "source": [
    "# texts = []\n",
    "\n",
    "# for link in tqdm(links):\n",
    "#     r = requests.get(link)\n",
    "#     if r.status_code != 200:\n",
    "#         continue\n",
    "#     page = html.fromstring(r.text)\n",
    "#     text = ''.join(page.xpath(\"//div[@id='article-content']/div[@class='body']//text()\"))\n",
    "#     texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gp5hSxn-esx4"
   },
   "outputs": [],
   "source": [
    "# with open('corpus_hum.txt', 'w') as f:\n",
    "#   for text in texts:\n",
    "#       if len(text) > 10:\n",
    "#           f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnycD_wdgR3V"
   },
   "source": [
    "## Corpus reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "w2qO8-JlfH66",
    "outputId": "bf804759-9354-4e50-9c06-93c68c390e19"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punctuation += '«»—…“”*№–'\n",
    "\n",
    "stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPA8BcO4hglE"
   },
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.cache = {}\n",
    "    \n",
    "    def lemmatize(self, token: str) -> str:\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        norm = self.morph.parse(token)[0].normal_form\n",
    "        self.cache[token] = norm\n",
    "        return norm\n",
    "\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def normalize(text: str) -> List[str]:\n",
    "    output = []\n",
    "    for token in word_tokenize(text.lower()):\n",
    "        token = token.strip(punctuation)\n",
    "        if not token:\n",
    "            continue\n",
    "            \n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        if lemma in stopwords:\n",
    "            continue\n",
    "            \n",
    "        output.append(lemma)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IiFE23Y3JM5L"
   },
   "outputs": [],
   "source": [
    "# with open('corpus_hum.txt', 'r') as f:\n",
    "#     corpus = f.read().splitlines()\n",
    "\n",
    "# corpus_norm = [normalize(text) for text in corpus]\n",
    "# corpus_norm = [text for text in corpus_norm if text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "_czLO7NuKcoM",
    "outputId": "710dd4d3-09e3-47a1-abee-456d9091f6c3"
   },
   "outputs": [],
   "source": [
    "# corpus_norm[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JEaVjJYxK0RG",
    "outputId": "dac365b2-82d1-42dd-dec3-60f619821ef1"
   },
   "outputs": [],
   "source": [
    "# len(corpus_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJEHeEAbql4E"
   },
   "source": [
    "## Training the fastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2duMLGKYLFex"
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from gensim.models import FastText\n",
    "\n",
    "# ft = FastText(corpus_norm, size=300, sg=1, negative=20, workers=4)\n",
    "# ft2 = FastText(corpus_norm, size=300, sg=1, negative=5, workers=4)\n",
    "# ft3 = FastText(corpus_norm, size=300, hs=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "iA9GgtMntriY",
    "outputId": "633f1ba7-2fa7-491b-b097-5e491116de8a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ft.save('ft/fasttext_hum.model')\n",
    "# ft2.save('ft/fasttext_hum_2.model')\n",
    "# ft3.save('ft/fasttext_hum_3.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svO9a10hqqMa"
   },
   "source": [
    "# Downstream task: paraphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "w2v = Word2Vec.load('w2v/word2vec_hum.model')\n",
    "ft = FastText.load('ft/fasttext_hum.model')\n",
    "ft2 = FastText.load('ft/fasttext_hum_2.model')\n",
    "ft3 = FastText.load('ft/fasttext_hum_3.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v2 = KeyedVectors.load_word2vec_format(\n",
    "    'news_upos_skipgram_300_5_2019/model.bin', binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "813UsE95wUvL"
   },
   "source": [
    "## Corpus parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2X52Sg0wquAJ"
   },
   "outputs": [],
   "source": [
    "def read_corpus(path: str) -> Dict[int, str]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        xml = f.read()\n",
    "    corpus = {}\n",
    "    text_pattern = re.compile(r'(?<=<value name=\"text\">).*?(?=</value>)', re.DOTALL)\n",
    "    id_pattern = re.compile(r'(?<=<value name=\"id\">).*?(?=</value>)', re.DOTALL)\n",
    "    sentence_pattern = re.compile(r'(?<=<sentence>).*?(?=</sentence>)', re.DOTALL)\n",
    "    for sentence in re.findall(sentence_pattern, xml):\n",
    "        corpus[int(re.search(id_pattern, sentence).group())] = \\\n",
    "            re.search(text_pattern, sentence).group()\n",
    "    return corpus\n",
    "\n",
    "def read_markup(path: str) -> List[Tuple[int]]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        xml = f.read()\n",
    "    markup = []\n",
    "    id_1_pattern = re.compile(r'(?<=<value name=\"id_1\">).*?(?=</value>)', re.DOTALL)\n",
    "    id_2_pattern = re.compile(r'(?<=<value name=\"id_2\">).*?(?=</value>)', re.DOTALL)\n",
    "    class_pattern = re.compile(r'(?<=<value name=\"class\">).*?(?=</value>)', re.DOTALL)\n",
    "    paraphrase_pattern = re.compile(r'(?<=<paraphrase>).*?(?=</paraphrase>)', re.DOTALL)\n",
    "    for paraphrase in re.findall(paraphrase_pattern, xml):\n",
    "        markup.append((int(re.search(id_1_pattern, paraphrase).group()),\n",
    "                       int(re.search(id_2_pattern, paraphrase).group()),\n",
    "                       int(re.search(class_pattern, paraphrase).group())))\n",
    "    return markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Knm89NMvtmu"
   },
   "outputs": [],
   "source": [
    "corpus = read_corpus('paraphraser/corpus.xml')\n",
    "markup = read_markup('paraphraser/paraphrases.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVPG0maOwmFh"
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "corpus_norm = {}\n",
    "\n",
    "for key, value in corpus.items():\n",
    "    tokens = normalize(value)\n",
    "    corpus_norm[key] = tokens\n",
    "    for token in tokens:\n",
    "        vocab.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10508"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "\n",
    "with open('ru-rnc.map.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        ms, ud = line.strip('\\n').split()\n",
    "        mapping[ms] = ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'ADJ',\n",
       " 'ADV': 'ADV',\n",
       " 'ADVPRO': 'ADV',\n",
       " 'ANUM': 'ADJ',\n",
       " 'APRO': 'DET',\n",
       " 'COM': 'ADJ',\n",
       " 'CONJ': 'SCONJ',\n",
       " 'INTJ': 'INTJ',\n",
       " 'NONLEX': 'X',\n",
       " 'NUM': 'NUM',\n",
       " 'PART': 'PART',\n",
       " 'PR': 'ADP',\n",
       " 'S': 'NOUN',\n",
       " 'SPRO': 'PRON',\n",
       " 'UNKN': 'X',\n",
       " 'V': 'VERB'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "def normalize_mystem(text: str) -> List[str]:\n",
    "    tokens = []\n",
    "    norm_words = m.analyze(text)\n",
    "    \n",
    "    for norm_word in norm_words:\n",
    "        if 'analysis' not in norm_word:\n",
    "            continue\n",
    "            \n",
    "        if not norm_word['analysis']:\n",
    "            lemma = norm_word['text']\n",
    "            pos = 'UNKN'\n",
    "        else:\n",
    "            lemma = norm_word['analysis'][0]['lex'].lower().strip()\n",
    "            pos = norm_word['analysis'][0]['gr'].split(',')[0].split('=')[0].strip()\n",
    "        if lemma in stopwords:\n",
    "            continue\n",
    "        pos = mapping[pos]\n",
    "        tokens.append(lemma + '_' + pos)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ms = set()\n",
    "corpus_ms = {}\n",
    "\n",
    "for key, value in corpus.items():\n",
    "    tokens = normalize_mystem(value)\n",
    "    corpus_ms[key] = tokens\n",
    "    for token in tokens:\n",
    "        vocab_ms.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9671"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "w2v_oov = np.mean([w2v.wv[token] for token in vocab if token in w2v.wv], axis=0)\n",
    "w2v2_oov = np.mean([w2v2.wv[token] for token in vocab_ms if token in w2v2.wv], axis=0)\n",
    "\n",
    "ft_oov = np.mean([ft.wv[token] for token in vocab if token in ft.wv], axis=0)\n",
    "ft2_oov = np.mean([ft2.wv[token] for token in vocab if token in ft2.wv], axis=0)\n",
    "ft3_oov = np.mean([ft3.wv[token] for token in vocab if token in ft3.wv], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(corpus: List[List[str]], model, oov: np.ndarray) -> np.ndarray:\n",
    "    output = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        output.append([])\n",
    "        for token in sentence:\n",
    "            embedding = model.wv[token] if token in model.wv else oov\n",
    "            output[-1].append(embedding)\n",
    "        output[-1] = np.average(output[-1], axis=0)\n",
    "        \n",
    "    return np.vstack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([x[2] for x in markup])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix decomposition embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy(x):\n",
    "    return x\n",
    "\n",
    "cv = CountVectorizer(max_features=100000, preprocessor=dummy, tokenizer=dummy)\n",
    "X_cv = cv.fit_transform(corpus_norm.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12062, 10508)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "nmf = NMF(300)\n",
    "nmf.fit(X_cv)\n",
    "\n",
    "svd = TruncatedSVD(300)\n",
    "svd.fit(X_cv)\n",
    "\n",
    "id2word = {i: w for i, w in enumerate(cv.get_feature_names())}\n",
    "word2id = {w: i for i, w in id2word.items()}\n",
    "\n",
    "id2vec_svd = nmf.components_.T\n",
    "id2vec_nmf = svd.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10508, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2vec_svd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10508, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2vec_nmf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "X = np.zeros((y.shape[0], 7))\n",
    "\n",
    "for i, (sent1_id, sent2_id, _) in enumerate(markup):\n",
    "    sent1, sent2 = corpus_norm[sent1_id], corpus_norm[sent2_id]\n",
    "    \n",
    "    sent1_w2v, sent2_w2v = vectorize([sent1, sent2], w2v, w2v_oov)\n",
    "    X[i, 0] = cosine_similarity(sent1_w2v.reshape(1, -1), sent2_w2v.reshape(1, -1))\n",
    "    \n",
    "    sent1_nmf = np.mean([id2vec_nmf[word2id[word]] for word in sent1], axis=0)\n",
    "    sent2_nmf = np.mean([id2vec_nmf[word2id[word]] for word in sent2], axis=0)\n",
    "    X[i, 2] = cosine_similarity(sent1_nmf.reshape(1, -1), sent2_nmf.reshape(1, -1))\n",
    "    \n",
    "    sent1_svd = np.mean([id2vec_svd[word2id[word]] for word in sent1], axis=0)\n",
    "    sent2_svd = np.mean([id2vec_svd[word2id[word]] for word in sent2], axis=0)\n",
    "    X[i, 3] = cosine_similarity(sent1_svd.reshape(1, -1), sent2_svd.reshape(1, -1))\n",
    "    \n",
    "    sent1_ft, sent2_ft = vectorize([sent1, sent2], ft, ft_oov)\n",
    "    X[i, 4] = cosine_similarity(sent1_ft.reshape(1, -1), sent2_ft.reshape(1, -1))\n",
    "    \n",
    "    sent1_ft2, sent2_ft2 = vectorize([sent1, sent2], ft2, ft2_oov)\n",
    "    X[i, 5] = cosine_similarity(sent1_ft2.reshape(1, -1), sent2_ft2.reshape(1, -1))\n",
    "    \n",
    "    sent1_ft3, sent2_ft3 = vectorize([sent1, sent2], ft3, ft3_oov)\n",
    "    X[i, 6] = cosine_similarity(sent1_ft3.reshape(1, -1), sent2_ft3.reshape(1, -1))\n",
    "    \n",
    "    sent1, sent2 = corpus_ms[sent1_id], corpus_ms[sent2_id]\n",
    "    sent1_w2v2, sent2_w2v2 = vectorize([sent1, sent2], w2v2, w2v2_oov)\n",
    "    X[i, 1] = cosine_similarity(sent1_w2v2.reshape(1, -1), sent2_w2v2.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WblBlKG7wdZa"
   },
   "source": [
    "## Logistic regression #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, X[:, :5], y, cv=10, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5512249568469924"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5508094427874269"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(logreg, X[:, [0, 1, 2, 3, 5]], y, cv=10, scoring='f1_micro')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.570184730316728"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(logreg, X[:, [0, 1, 2, 3, 6]], y, cv=10, scoring='f1_micro')\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that using different cosine distances between the same pair of texts vectorized with 5 different algorithms leads to a better f1 metric than using concatenated means of word vectors. Out of the three fastText models we trained, the third one, which uses hierarchical softmax, is the best one. The two models trained using negative sampling perform roughly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
