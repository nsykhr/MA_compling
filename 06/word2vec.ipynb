{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydtKaWvuwHx7"
   },
   "source": [
    "# Training word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wC2JCaZpdlnV"
   },
   "source": [
    "## Corpus parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oV7AlZP9dJ6C"
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from tqdm import tqdm\n",
    "# from time import sleep\n",
    "# from lxml import etree, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5y5a6J_ndn2e"
   },
   "outputs": [],
   "source": [
    "# links = []\n",
    "\n",
    "# for topic in [33, 37, 10, 13, 36, 12, 34, 39]:\n",
    "#     for letter in 'абвгдезиклмнпростуфхцшщчуэюя':\n",
    "#         for page_num in range(30):\n",
    "#             r = requests.get(f'https://www.krugosvet.ru/taxonomy/term/{topic}/{letter}?page={page_num}')\n",
    "#             if r.status_code != 200:\n",
    "#                 continue\n",
    "#             page = html.fromstring(r.text)\n",
    "#             hrefs = page.xpath(\"//div[@class='article-teaser']/a/@href\")\n",
    "#             links += ['https://www.krugosvet.ru'+href for href in hrefs]\n",
    "            \n",
    "#     print(f'After topic: {topic}, links collected: {len(links)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87x27UGZeEoA"
   },
   "outputs": [],
   "source": [
    "# texts = []\n",
    "\n",
    "# for link in tqdm(links):\n",
    "#     r = requests.get(link)\n",
    "#     if r.status_code != 200:\n",
    "#         continue\n",
    "#     page = html.fromstring(r.text)\n",
    "#     text = ''.join(page.xpath(\"//div[@id='article-content']/div[@class='body']//text()\"))\n",
    "#     texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gp5hSxn-esx4"
   },
   "outputs": [],
   "source": [
    "# with open('corpus_hum.txt', 'w') as f:\n",
    "#   for text in texts:\n",
    "#       if len(text) > 10:\n",
    "#           f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnycD_wdgR3V"
   },
   "source": [
    "## Corpus reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "w2qO8-JlfH66",
    "outputId": "bf804759-9354-4e50-9c06-93c68c390e19"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punctuation += '«»—…“”*№–'\n",
    "\n",
    "stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPA8BcO4hglE"
   },
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.cache = {}\n",
    "    \n",
    "    def lemmatize(self, token: str) -> str:\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        norm = self.morph.parse(token)[0].normal_form\n",
    "        self.cache[token] = norm\n",
    "        return norm\n",
    "\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "def normalize(text: str) -> List[str]:\n",
    "    output = []\n",
    "    for token in word_tokenize(text.lower()):\n",
    "        token = token.strip(punctuation)\n",
    "        if not token:\n",
    "            continue\n",
    "            \n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        if lemma in stopwords:\n",
    "            continue\n",
    "            \n",
    "        output.append(lemma)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IiFE23Y3JM5L"
   },
   "outputs": [],
   "source": [
    "with open('corpus_hum.txt', 'r') as f:\n",
    "    corpus = f.read().splitlines()\n",
    "\n",
    "corpus_norm = [normalize(text) for text in corpus]\n",
    "corpus_norm = [text for text in corpus_norm if text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "_czLO7NuKcoM",
    "outputId": "710dd4d3-09e3-47a1-abee-456d9091f6c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['абай',\n",
       " 'василий',\n",
       " 'васо',\n",
       " 'иван',\n",
       " '1900–2001',\n",
       " 'русский',\n",
       " 'лингвист',\n",
       " 'родиться',\n",
       " '2',\n",
       " '15']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_norm[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JEaVjJYxK0RG",
    "outputId": "dac365b2-82d1-42dd-dec3-60f619821ef1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115075"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DJEHeEAbql4E"
   },
   "source": [
    "## Training the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2duMLGKYLFex"
   },
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# w2v = Word2Vec(corpus_norm, size=300, sg=1, negative=20, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "iA9GgtMntriY",
    "outputId": "633f1ba7-2fa7-491b-b097-5e491116de8a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# w2v.save('w2v/word2vec_hum.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v = Word2Vec.load('w2v/word2vec_hum.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svO9a10hqqMa"
   },
   "source": [
    "# Downstream task: paraphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "813UsE95wUvL"
   },
   "source": [
    "## Corpus parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2X52Sg0wquAJ"
   },
   "outputs": [],
   "source": [
    "def read_corpus(path: str) -> Dict[int, str]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        xml = f.read()\n",
    "    corpus = {}\n",
    "    text_pattern = re.compile(r'(?<=<value name=\"text\">).*?(?=</value>)', re.DOTALL)\n",
    "    id_pattern = re.compile(r'(?<=<value name=\"id\">).*?(?=</value>)', re.DOTALL)\n",
    "    sentence_pattern = re.compile(r'(?<=<sentence>).*?(?=</sentence>)', re.DOTALL)\n",
    "    for sentence in re.findall(sentence_pattern, xml):\n",
    "        corpus[int(re.search(id_pattern, sentence).group())] = \\\n",
    "            re.search(text_pattern, sentence).group()\n",
    "    return corpus\n",
    "\n",
    "def read_markup(path: str) -> List[Tuple[int]]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        xml = f.read()\n",
    "    markup = []\n",
    "    id_1_pattern = re.compile(r'(?<=<value name=\"id_1\">).*?(?=</value>)', re.DOTALL)\n",
    "    id_2_pattern = re.compile(r'(?<=<value name=\"id_2\">).*?(?=</value>)', re.DOTALL)\n",
    "    class_pattern = re.compile(r'(?<=<value name=\"class\">).*?(?=</value>)', re.DOTALL)\n",
    "    paraphrase_pattern = re.compile(r'(?<=<paraphrase>).*?(?=</paraphrase>)', re.DOTALL)\n",
    "    for paraphrase in re.findall(paraphrase_pattern, xml):\n",
    "        markup.append((int(re.search(id_1_pattern, paraphrase).group()),\n",
    "                       int(re.search(id_2_pattern, paraphrase).group()),\n",
    "                       int(re.search(class_pattern, paraphrase).group())))\n",
    "    return markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Knm89NMvtmu"
   },
   "outputs": [],
   "source": [
    "corpus = read_corpus('paraphraser/corpus.xml')\n",
    "markup = read_markup('paraphraser/paraphrases.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVPG0maOwmFh"
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "corpus_norm = {}\n",
    "\n",
    "for key, value in corpus.items():\n",
    "    tokens = normalize(value)\n",
    "    corpus_norm[key] = tokens\n",
    "    for token in tokens:\n",
    "        vocab.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10509"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov = np.mean([w2v.wv[token] for token in vocab if token in w2v.wv], axis=0)\n",
    "oov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# def dummy(x):\n",
    "#     return x\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(analyzer='word', preprocessor=dummy,\n",
    "#                                    tokenizer=dummy, ngram_range=(1, 1),\n",
    "#                                    max_df = 1.0, min_df = 0)\n",
    "# tfidf_vectorizer.fit(corpus_norm.values())\n",
    "\n",
    "# token2idf = {}\n",
    "# for i, token in enumerate(tfidf_vectorizer.get_feature_names()):\n",
    "#     token2idf[token] = tfidf_vectorizer.idf_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import softmax\n",
    "\n",
    "def vectorize(corpus: List[List[str]]) -> np.ndarray:\n",
    "    output = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        idfs = []\n",
    "        output.append([])\n",
    "        for token in sentence:\n",
    "            # idfs.append(token2idf[token])\n",
    "            embedding = w2v.wv[token] if token in w2v.wv else oov\n",
    "            output[-1].append(embedding)\n",
    "        output[-1] = np.average(output[-1], axis=0) #, weights=softmax(idfs))\n",
    "        \n",
    "    return np.vstack(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P. S. I decided to not use the tf-idf weights after all because it made the f-score worse for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 600)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.hstack((\n",
    "    vectorize([corpus_norm[x[0]] for x in markup]),\n",
    "    vectorize([corpus_norm[x[1]] for x in markup])\n",
    "))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([x[2] for x in markup])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WblBlKG7wdZa"
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, X, y, cv=10, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4374850983363685"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckJ9lTaCqvJ4"
   },
   "source": [
    "# External pretrained embeddings for paraphrase identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(\n",
    "    'news_upos_skipgram_300_5_2019/model.bin', binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "\n",
    "with open('ru-rnc.map.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        ms, ud = line.strip('\\n').split()\n",
    "        mapping[ms] = ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 'ADJ',\n",
       " 'ADV': 'ADV',\n",
       " 'ADVPRO': 'ADV',\n",
       " 'ANUM': 'ADJ',\n",
       " 'APRO': 'DET',\n",
       " 'COM': 'ADJ',\n",
       " 'CONJ': 'SCONJ',\n",
       " 'INTJ': 'INTJ',\n",
       " 'NONLEX': 'X',\n",
       " 'NUM': 'NUM',\n",
       " 'PART': 'PART',\n",
       " 'PR': 'ADP',\n",
       " 'S': 'NOUN',\n",
       " 'SPRO': 'PRON',\n",
       " 'UNKN': 'X',\n",
       " 'V': 'VERB'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "def normalize_mystem(text: str) -> List[str]:\n",
    "    tokens = []\n",
    "    norm_words = m.analyze(text)\n",
    "    \n",
    "    for norm_word in norm_words:\n",
    "        if 'analysis' not in norm_word or not norm_word['analysis']:\n",
    "            continue\n",
    "        lemma = norm_word['analysis'][0]['lex'].lower().strip()\n",
    "        if lemma in stopwords:\n",
    "            continue\n",
    "        pos = norm_word['analysis'][0]['gr'].split(',')[0].split('=')[0].strip()\n",
    "        pos = mapping[pos]\n",
    "        tokens.append(lemma + '_' + pos)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "corpus_norm = {}\n",
    "\n",
    "for key, value in corpus.items():\n",
    "    tokens = normalize_mystem(value)\n",
    "    corpus_norm[key] = tokens\n",
    "    for token in tokens:\n",
    "        vocab.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9211"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "oov = np.mean([w2v.wv[token] for token in vocab if token in w2v.wv], axis=0)\n",
    "oov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 600)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.hstack((\n",
    "    vectorize([corpus_norm[x[0]] for x in markup]),\n",
    "    vectorize([corpus_norm[x[1]] for x in markup])\n",
    "))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([x[2] for x in markup])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, X, y, cv=10, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4236451618490908"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the results that the heavier embeddings precomputed on a large corpus work, surprisingly, a little worse on this downstream task, but the difference is not dramatic. The embeddings are trained on news data, and in the downstream task, the examples are drawn from news as well, while our own embeddings are trained on humanitarian articles. It is difficult to say why our embeddings perform better on this task compared to the external embeddings — perhaps it has something to do with the choice of the hyperparameters?\n",
    "\n",
    "Nevertheless, it is clear that a difficult semantic task like this one should be solved with more complex architectures: neural networks. Logistic regression on top of the means of word embeddings just isn't good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
