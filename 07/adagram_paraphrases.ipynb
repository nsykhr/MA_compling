{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adagram\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from typing import List, Tuple, Dict, Iterator\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "punctuation += '«»—…“”*№–'\n",
    "stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.cache = {}\n",
    "    \n",
    "    def lemmatize(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        norm = self.morph.parse(token)[0].normal_form\n",
    "        self.cache[token] = norm\n",
    "        return norm\n",
    "\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> List[str]:\n",
    "    output = []\n",
    "    for token in word_tokenize(text.lower()):\n",
    "        token = token.strip(punctuation)\n",
    "        if not token:\n",
    "            continue\n",
    "            \n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        if lemma in stopwords:\n",
    "            continue\n",
    "            \n",
    "        output.append(lemma)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path: str) -> Dict[int, str]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        xml = f.read()\n",
    "    corpus = {}\n",
    "    text_pattern = re.compile(r'(?<=<value name=\"text\">).*?(?=</value>)', re.DOTALL)\n",
    "    id_pattern = re.compile(r'(?<=<value name=\"id\">).*?(?=</value>)', re.DOTALL)\n",
    "    sentence_pattern = re.compile(r'(?<=<sentence>).*?(?=</sentence>)', re.DOTALL)\n",
    "    for sentence in re.findall(sentence_pattern, xml):\n",
    "        corpus[int(re.search(id_pattern, sentence).group())] = \\\n",
    "            re.search(text_pattern, sentence).group()\n",
    "    return corpus\n",
    "\n",
    "def read_markup(path: str) -> List[Tuple[int]]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        xml = f.read()\n",
    "    markup = []\n",
    "    id_1_pattern = re.compile(r'(?<=<value name=\"id_1\">).*?(?=</value>)', re.DOTALL)\n",
    "    id_2_pattern = re.compile(r'(?<=<value name=\"id_2\">).*?(?=</value>)', re.DOTALL)\n",
    "    class_pattern = re.compile(r'(?<=<value name=\"class\">).*?(?=</value>)', re.DOTALL)\n",
    "    paraphrase_pattern = re.compile(r'(?<=<paraphrase>).*?(?=</paraphrase>)', re.DOTALL)\n",
    "    for paraphrase in re.findall(paraphrase_pattern, xml):\n",
    "        markup.append((int(re.search(id_1_pattern, paraphrase).group()),\n",
    "                       int(re.search(id_2_pattern, paraphrase).group()),\n",
    "                       int(re.search(class_pattern, paraphrase).group())))\n",
    "    return markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus('paraphraser/corpus.xml')\n",
    "markup = read_markup('paraphraser/paraphrases.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_norm = {}\n",
    "\n",
    "for key, value in corpus.items():\n",
    "    tokens = normalize(value)\n",
    "    corpus_norm[key] = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adagram text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load('out.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_embedding(token: str, context: List[str]) -> np.ndarray:\n",
    "    try:\n",
    "        return vm.sense_vector(token, vm.disambiguate(token, context).argmax())\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(tokens: List[str], window_size: int = 5) -> Iterator[Tuple[str, List[str]]]:\n",
    "    return (\n",
    "        (token, tokens[max(0, i-window_size):i] + tokens[i+1:i+window_size+1])\n",
    "        for i, token in enumerate(tokens)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(tokens: List[str]) -> np.ndarray:\n",
    "    token_vectors = []\n",
    "    \n",
    "    for token, context in get_windows(tokens):\n",
    "        token_embedding = get_token_embedding(token, context)\n",
    "        \n",
    "        if token_embedding is not None:\n",
    "            token_vectors.append(token_embedding)\n",
    "    \n",
    "    return np.mean(token_vectors, axis=0) if token_vectors else np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 200)\n",
      "(7227,)\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for idx1, idx2, label in markup:\n",
    "    X.append(np.hstack(\n",
    "        (vectorize_sentence(corpus_norm[idx1]), vectorize_sentence(corpus_norm[idx2]))\n",
    "    ))\n",
    "    y.append(label)\n",
    "\n",
    "X = np.vstack(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape, y.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, X, y, cv=10, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4219869725065538"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would appear that concatenating averaged Adagram vectors doesn't work quite so well. Tweaking the window size parameter doesn't seem to help, either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
