{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is as follows: given a word in some context, we need to encode the context and compare it to the example contexts from WordNet. Having calculated some measure of semantic similarity, we then need to choose the best synset from WordNet for this word and then compare it to the right one. For computing sentence similarity, we are going to use a) Lesk's algorithm; b) Google AI's Universal Sentence Encoder Model (USE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/master/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import stanfordnlp\n",
    "# stanfordnlp.download('en')\n",
    "\n",
    "punctuation += '«»—…“”*№–'\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[]]\n",
    "\n",
    "with open('corpus_wsd_50k.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            corpus.append([])\n",
    "            continue\n",
    "        \n",
    "        corpus[-1].append(tuple(line.strip().split('\\t')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesk algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: Union[str, List[str]]) -> List[str]:\n",
    "    if isinstance(text, list):\n",
    "        return text\n",
    "    \n",
    "    output = []\n",
    "    for token in word_tokenize(text.lower()):\n",
    "        token = token.strip(punctuation)\n",
    "        if not token or token in stopwords:\n",
    "            continue\n",
    "        output.append(token)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap(sent1: Union[str, List[str]], sent2: Union[str, List[str]]) -> int:\n",
    "    tokens1, tokens2 = tuple(map(normalize, (sent1, sent2)))\n",
    "    return len(set(tokens1) & set(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_synset_lesk(word: str, context: List[str]) -> int:\n",
    "    best_sense = 0\n",
    "    best_overlap = 0\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        for example in synset.examples():\n",
    "            overlap = get_overlap(context, example)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_sense = i\n",
    "    \n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:42<00:00, 23.43it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracy, total = 0, 0\n",
    "\n",
    "for instance in tqdm(corpus[:1000]):\n",
    "    context = list(map(lambda x: x[-1], instance))\n",
    "    \n",
    "    for word in instance:\n",
    "        if len(word) == 3:\n",
    "            true = wn.lemma_from_key(word[0]).synset()\n",
    "            pred = wn.synsets(word[1])[find_best_synset_lesk(word[1], context)]\n",
    "            \n",
    "            if true == pred:\n",
    "                accuracy += 1\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesk algorithm: 53.04% of disambiguations are correct.\n"
     ]
    }
   ],
   "source": [
    "print(f'Lesk algorithm: {accuracy / total * 100:.2f}% of disambiguations are correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that in its current form, our implementation of Lesk's algorithm doesn't take POS tags into account. Let's fix that to try and improve our metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesk algorithm + StanfordNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use this library (known for its good performance) to find POS tags and lemmas. Hopefully finding intersections between sets of lemmas and constraining our search with the right POS tags will help the metric rise. Moreover, let's also take into account tokens that are repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/master/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'pretokenized': True, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/master/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/master/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/master/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'pretokenized': False, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/master/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp_pos = stanfordnlp.Pipeline(processors='tokenize,pos', tokenize_pretokenized=True)\n",
    "nlp_lemmas = stanfordnlp.Pipeline(processors='tokenize,lemma', tokenize_pretokenized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "\n",
    "def normalize(text: Union[str, List[str]]) -> List[str]:\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    if text in cache:\n",
    "        return cache[text]\n",
    "    \n",
    "    normalized = [token[2] for sent in nlp_lemmas(text).conll_file.sents for token in sent\n",
    "                  if token[2].strip(punctuation) and token[2] not in stopwords]\n",
    "    cache[text] = normalized\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap(sent1: Union[str, List[str]], sent2: Union[str, List[str]]) -> int:\n",
    "    tokens1, tokens2 = tuple(map(normalize, (sent1, sent2)))\n",
    "    return sum(1 for token in tokens1 if token in tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not take adjective POS tags into account\n",
    "# because there are two different kinds of adjectives in WordNet.\n",
    "# UD tagging format does not differentiate between them.\n",
    "\n",
    "pos_alignment = {\n",
    "    'NOUN': 'n',\n",
    "    'PROPN': 'n',\n",
    "    'VERB': 'v',\n",
    "    'AUX': 'v',\n",
    "    'ADV': 'r'\n",
    "}\n",
    "\n",
    "def find_best_synset_lesk(word: str, pos: str, context: List[str]) -> int:\n",
    "    best_sense = 0\n",
    "    best_overlap = 0\n",
    "        \n",
    "    args = [word]\n",
    "    \n",
    "    if pos in pos_alignment:\n",
    "        args.append(pos_alignment[pos])\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(*args)):\n",
    "        for example in synset.examples():\n",
    "            overlap = get_overlap(context, example)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_sense = i\n",
    "    \n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [07:15<00:00,  2.30it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracy, total = 0, 0\n",
    "\n",
    "for instance in tqdm(corpus[:1000]):\n",
    "    context = list(map(lambda x: x[-1], instance))\n",
    "    pos_tags = [token[3] for token in nlp_pos([context]).conll_file.sents[0]]\n",
    "    \n",
    "    for word, pos in zip(instance, pos_tags):\n",
    "        if len(word) == 3:\n",
    "            true = wn.lemma_from_key(word[0]).synset()\n",
    "            pred = wn.synsets(word[1])[find_best_synset_lesk(word[1], pos, context)]\n",
    "            \n",
    "            if true == pred:\n",
    "                accuracy += 1\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesk algorithm: 51.25% of disambiguations are correct.\n"
     ]
    }
   ],
   "source": [
    "print(f'Lesk algorithm: {accuracy / total * 100:.2f}% of disambiguations are correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, using StanfordNLP lemmatization and POS tags and tweaking the overlap computation algorithm didn't help. I conducted some ablation studies, and just StanfordNLP lemmatization+POS or just lemmatization didn't improve the metric, either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder by Google AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library tensorflow-hub offers a convenient API to get sentence embeddings from USE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading module https://tfhub.dev/google/universal-sentence-encoder-large/5 ...\n",
      "Module https://tfhub.dev/google/universal-sentence-encoder-large/5 is loaded.\n"
     ]
    }
   ],
   "source": [
    "class USEForWordNet:\n",
    "    def __init__(self, module_url: str = 'https://tfhub.dev/google/universal-sentence-encoder-large/5'):\n",
    "        print(f'Loading module {module_url} ...')\n",
    "        self.model = hub.load(module_url)\n",
    "        print(f'Module {module_url} is loaded.')\n",
    "        self.cache = {}\n",
    "        \n",
    "    def embed(self, sentences: List[str]) -> np.ndarray:\n",
    "        return np.array(self.model(sentences))\n",
    "    \n",
    "    def get_synset_embeddings(self, word: str) -> np.ndarray:\n",
    "        if word in self.cache:\n",
    "            return self.cache[word]\n",
    "        \n",
    "        synset_embeddings = np.vstack(\n",
    "            [np.mean(\n",
    "                self.embed(synset.examples()),\n",
    "                axis=0\n",
    "            ) for synset in wn.synsets(word)]\n",
    "        )\n",
    "        self.cache[word] = synset_embeddings\n",
    "        \n",
    "        return synset_embeddings\n",
    "\n",
    "use_for_wn = USEForWordNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_synset_use(word: str, context: str) -> int:\n",
    "    synset_embeddings = use_for_wn.get_synset_embeddings(word)\n",
    "    context_use = use_for_wn.embed([context])\n",
    "\n",
    "    return cdist(context_use, synset_embeddings, metric='euclidean')[0].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [39:16<00:00,  2.36s/it]   \n"
     ]
    }
   ],
   "source": [
    "accuracy, total = 0, 0\n",
    "\n",
    "for instance in tqdm(corpus[:1000]):\n",
    "    context = ' '.join(list(map(lambda x: x[-1], instance)))\n",
    "    \n",
    "    for word in instance:\n",
    "        if len(word) == 3:\n",
    "            true = wn.lemma_from_key(word[0]).synset()\n",
    "            pred = wn.synsets(word[1])[find_best_synset_use(word[1], context)]\n",
    "            \n",
    "            if true == pred:\n",
    "                accuracy += 1\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Sentence Encoder: 39.79% of disambiguations are correct.\n"
     ]
    }
   ],
   "source": [
    "print(f'Universal Sentence Encoder: {accuracy / total * 100:.2f}% of disambiguations are correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE is completely useless at this task (type of distance metric doesn't affect the result much), perhaps because the task itself is ill-defined: vector representations of our context and the example context do not have to be similar. Rather, it's contextual embeddings for the token that we're interested in should be similar. For this we should be using Google AI's BERT, but let's just say further experimentation is outside the scope of this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('how', 'How'),\n",
       " ('long%3:00:02::', 'long', 'long'),\n",
       " ('have', 'has'),\n",
       " ('it', 'it'),\n",
       " ('be%2:42:03::', 'be', 'been'),\n",
       " ('since', 'since'),\n",
       " ('you', 'you'),\n",
       " ('review%2:31:00::', 'review', 'reviewed'),\n",
       " ('the', 'the'),\n",
       " ('objective%1:09:00::', 'objective', 'objectives'),\n",
       " ('of', 'of'),\n",
       " ('you', 'your'),\n",
       " ('benefit%1:21:00::', 'benefit', 'benefit'),\n",
       " ('and', 'and'),\n",
       " ('service%1:04:07::', 'service', 'service'),\n",
       " ('program%1:09:01::', 'program', 'program'),\n",
       " ('?', '?')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
