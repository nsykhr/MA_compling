{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is as follows: given a word in some context, we need to encode the context and compare it to the example contexts from WordNet. Having calculated some measure of semantic similarity, we then need to choose the best synset from WordNet for this word and then compare it to the right one. For computing sentence similarity, we are going to use a) Lesk's algorithm; b) Google AI's Universal Sentence Encoder Model (USE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/master/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import stanfordnlp\n",
    "# stanfordnlp.download('en')\n",
    "\n",
    "punctuation += '«»—…“”*№–'\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[]]\n",
    "\n",
    "with open('corpus_wsd_50k.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            corpus.append([])\n",
    "            continue\n",
    "        \n",
    "        corpus[-1].append(tuple(line.strip().split('\\t')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesk's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: Union[str, List[str]]) -> List[str]:\n",
    "    if isinstance(text, list):\n",
    "        return text\n",
    "    \n",
    "    output = []\n",
    "    for token in word_tokenize(text.lower()):\n",
    "        token = token.strip(punctuation)\n",
    "        if not token or token in stopwords:\n",
    "            continue\n",
    "        output.append(token)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlap(sent1: Union[str, List[str]], sent2: Union[str, List[str]]) -> int:\n",
    "    tokens1, tokens2 = tuple(map(normalize, (sent1, sent2)))\n",
    "    return len(set(tokens1) & set(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_synset_lesk(word: str, context: List[str]) -> int:\n",
    "    best_sense = 0\n",
    "    best_overlap = 0\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        for example in synset.examples():\n",
    "            overlap = get_overlap(context, example)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_sense = i\n",
    "    \n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:44<00:00, 22.44it/s]\n"
     ]
    }
   ],
   "source": [
    "accuracy, total = 0, 0\n",
    "\n",
    "for instance in tqdm(corpus[:1000]):\n",
    "    context = list(map(lambda x: x[-1], instance))\n",
    "    \n",
    "    for word in instance:\n",
    "        if len(word) == 3:\n",
    "            true = wn.lemma_from_key(word[0]).synset()\n",
    "            pred = wn.synsets(word[1])[find_best_synset_lesk(word[1], context)]\n",
    "            \n",
    "            if true == pred:\n",
    "                accuracy += 1\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesk's algorithm: 53.04% of disambiguations are correct.\n"
     ]
    }
   ],
   "source": [
    "print(f'Lesk\\'s algorithm: {accuracy / total * 100:.2f}% of disambiguations are correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that in its current form, our implementation of Lesk's algorithm doesn't take POS tags into account. Let's fix that to try and improve our metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesk's algorithm + StanfordNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use this library (known for its good performance) to find POS tags and lemmas. Hopefully finding intersections between sets of lemmas and constraining our search with the right POS tags will help the metric rise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/master/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'pretokenized': True, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/master/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/master/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/master/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(processors='tokenize,lemma,pos', tokenize_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "\n",
    "def normalize(text: Union[str, List[str]]) -> List[str]:\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    if tuple(text) in cache:\n",
    "        return cache[tuple(text)]\n",
    "    \n",
    "    normalized = [token[2] for token in nlp([text]).conll_file.sents[0]\n",
    "                  if token[2].strip(punctuation)]\n",
    "    cache[tuple(text)] = normalized\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_alignment = {\n",
    "    'NOUN': 'n',\n",
    "    'PROPN': 'n',\n",
    "    'PRON': 'n',\n",
    "    'VERB': 'v',\n",
    "    'AUX': 'v',\n",
    "    'ADV': 'r'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_synset_lesk(word: str, pos: str, context: List[str]) -> int:\n",
    "    best_sense = 0\n",
    "    best_overlap = 0\n",
    "        \n",
    "    args = [word]\n",
    "    \n",
    "    # We do not take adjective POS tags into account\n",
    "    # because there are two different kinds of adjectives in WordNet.\n",
    "    # UD tagging format does not differentiate between them.\n",
    "    if pos in pos_alignment:\n",
    "        args.append(pos_alignment[pos])\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(*args)):\n",
    "        for example in synset.examples():\n",
    "            overlap = get_overlap(context, example)\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_sense = i\n",
    "    \n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [17:02<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "accuracy, total = 0, 0\n",
    "\n",
    "for instance in tqdm(corpus[:1000]):\n",
    "    context = list(map(lambda x: x[-1], instance))\n",
    "    pos_tags = [token[3] for token in nlp([context]).conll_file.sents[0]]\n",
    "    \n",
    "    for word, pos in zip(instance, pos_tags):\n",
    "        if len(word) == 3:\n",
    "            true = wn.lemma_from_key(word[0]).synset()\n",
    "            pred = wn.synsets(word[1])[find_best_synset_lesk(word[1], pos, context)]\n",
    "            \n",
    "            if true == pred:\n",
    "                accuracy += 1\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lesk's algorithm: 49.10% of disambiguations are correct.\n"
     ]
    }
   ],
   "source": [
    "print(f'Lesk\\'s algorithm: {accuracy / total * 100:.2f}% of disambiguations are correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder by Google AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library tensorflow-hub offers a convenient API to get sentence embeddings from USE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading module https://tfhub.dev/google/universal-sentence-encoder-large/5 ...\n",
      "Module https://tfhub.dev/google/universal-sentence-encoder-large/5 is loaded.\n"
     ]
    }
   ],
   "source": [
    "class USEForWordNet:\n",
    "    def __init__(self, module_url: str = 'https://tfhub.dev/google/universal-sentence-encoder-large/5'):\n",
    "        print(f'Loading module {module_url} ...')\n",
    "        self.model = hub.load(module_url)\n",
    "        print(f'Module {module_url} is loaded.')\n",
    "        self.cache = {}\n",
    "        \n",
    "    def embed(self, sentences: List[str]) -> np.ndarray:\n",
    "        return np.array(self.model(sentences))\n",
    "    \n",
    "    def get_synset_embeddings(self, word: str) -> np.ndarray:\n",
    "        if word in self.cache:\n",
    "            return self.cache[word]\n",
    "        \n",
    "        synset_embeddings = np.vstack(\n",
    "            [np.mean(\n",
    "                self.embed(synset.examples()),\n",
    "                axis=0\n",
    "            ) for synset in wn.synsets(word)]\n",
    "        )\n",
    "        self.cache[word] = synset_embeddings\n",
    "        \n",
    "        return synset_embeddings\n",
    "\n",
    "use_for_wn = USEForWordNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_synset_use(word: str, context: str) -> int:\n",
    "    synset_embeddings = use_for_wn.get_synset_embeddings(word)\n",
    "    context_use = use_for_wn.embed([context])\n",
    "\n",
    "    return cdist(context_use, synset_embeddings, metric='cosine')[0].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/1000 [01:12<1:09:21,  4.22s/it]ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-b6a11bbaaa26>\", line 9, in <module>\n",
      "    pred = wn.synsets(word[1])[find_best_synset_use(word[1], context)]\n",
      "  File \"<ipython-input-15-c3a7bebbc105>\", line 2, in find_best_synset_use\n",
      "    synset_embeddings = use_for_wn.get_synset_embeddings(word)\n",
      "  File \"<ipython-input-14-5afee7edaa2b>\", line 19, in get_synset_embeddings\n",
      "    ) for synset in wn.synsets(word)]\n",
      "  File \"<ipython-input-14-5afee7edaa2b>\", line 19, in <listcomp>\n",
      "    ) for synset in wn.synsets(word)]\n",
      "  File \"<ipython-input-14-5afee7edaa2b>\", line 9, in embed\n",
      "    return np.array(self.model(sentences))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py\", line 438, in _call_attribute\n",
      "    return instance.__call__(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 606, in _call\n",
      "    results = self._stateful_fn(*args, **kwds)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2363, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\n",
      "    self.captured_inputs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 545, in call\n",
      "    ctx=ctx)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\n",
      "    num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "accuracy, total = 0, 0\n",
    "\n",
    "for instance in tqdm(corpus[:1000]):\n",
    "    context = ' '.join(list(map(lambda x: x[-1], instance)))\n",
    "    \n",
    "    for word in instance:\n",
    "        if len(word) == 3:\n",
    "            true = wn.lemma_from_key(word[0]).synset()\n",
    "            pred = wn.synsets(word[1])[find_best_synset_use(word[1], context)]\n",
    "            \n",
    "            if true == pred:\n",
    "                accuracy += 1\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Universal Sentence Encoder: {accuracy / total * 100:.2f}% of disambiguations are correct.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
