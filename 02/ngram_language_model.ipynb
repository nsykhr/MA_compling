{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Энграммная языковая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс для нашей языковой модели, в котором будет реализован end-to-end пайплайн: чтение текстовых файлов, препроцессинг, подсчёт частотностей энграмм, построение матрицы вероятностей переходов, случайная генерация текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import *\n",
    "from string import punctuation\n",
    "from scipy.special import softmax\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "punctuation += '«»–—' # Расширяем строку punctuation кавычками-ёлочками и коротким и длинным тире.\n",
    "\n",
    "class NgramLanguageModel:\n",
    "    def __init__(self, path: str, n: int = 2) -> None:\n",
    "        if n < 2:\n",
    "            # Строго говоря, может, но униграммные модели в этом ноутбуке я решил не рассматривать.\n",
    "            raise Exception('Параметр N энграммной языковой модели не может быть меньше 2!')\n",
    "        self.n = n\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            self.text = f.read()\n",
    "        self.sentences = map(lambda sentence: ['<START>'] * (self.n - 1) + \\\n",
    "                             [x for x in self.tokenize(sentence) if x] + ['<END>'],\n",
    "                             sent_tokenize(self.text))\n",
    "        self.token2id = {}\n",
    "        if self.n > 2:\n",
    "            self.ngram2id = {}\n",
    "        self.ngrams = [Counter(), Counter()]\n",
    "        self.get_ngram_counts()\n",
    "        self.vocab_size = len(self.token2id)\n",
    "        self.id2token = {value: key for key, value in self.token2id.items()}\n",
    "        self.matrix = np.zeros((len(self.ngrams[0]), self.vocab_size))\n",
    "        if self.n > 2:\n",
    "            self.id2ngram = {value: key for key, value in self.ngram2id.items()}\n",
    "        self.populate_matrix()\n",
    "    \n",
    "    def get_ngram_counts(self) -> None:\n",
    "        if self.n == 2:\n",
    "            for sentence in self.sentences:\n",
    "                self.ngrams[0].update(sentence)\n",
    "                self.ngrams[1].update(list(self.ngrammer(sentence, 2)))\n",
    "                self.update_token2id(sentence)\n",
    "        if self.n > 2:\n",
    "            for sentence in self.sentences:\n",
    "                ngrams = list(self.ngrammer(sentence, self.n-1))\n",
    "                self.ngrams[0].update(ngrams)\n",
    "                self.ngrams[1].update(list(self.ngrammer(sentence, self.n)))\n",
    "                self.update_token2id(sentence)\n",
    "                self.update_ngram2id(ngrams)\n",
    "    \n",
    "    def update_token2id(self, tokens: List[str]) -> None:\n",
    "        for token in tokens:\n",
    "            if token not in self.token2id:\n",
    "                self.token2id[token] = len(self.token2id)\n",
    "    \n",
    "    def update_ngram2id(self, ngrams: List[str]) -> None:\n",
    "        for ngram in ngrams:\n",
    "            if ngram not in self.ngram2id:\n",
    "                self.ngram2id[ngram] = len(self.ngram2id)\n",
    "    \n",
    "    def populate_matrix(self):\n",
    "        if self.n == 2:\n",
    "            for ngram, count in self.ngrams[1].items():\n",
    "                src, dest = ngram.split()\n",
    "                self.matrix[self.token2id[src], self.token2id[dest]] = \\\n",
    "                    count / self.ngrams[0][src]\n",
    "        if self.n > 2:\n",
    "            for ngram, count in self.ngrams[1].items():\n",
    "                ngram_splitted = ngram.split()\n",
    "                src = ' '.join(ngram_splitted[:-1])\n",
    "                dest = ngram_splitted[-1]\n",
    "                self.matrix[self.ngram2id[src], self.token2id[dest]] = \\\n",
    "                    count / self.ngrams[0][src]\n",
    "\n",
    "    def generate_random_text(self, n: int = 100,\n",
    "                             random_state: int = None) -> Generator[str, None, None]:\n",
    "        if self.n == 2:\n",
    "            current_idx = self.token2id['<START>']\n",
    "            for i in range(n):\n",
    "                if random_state:\n",
    "                    np.random.seed(random_state)\n",
    "                chosen = np.random.choice(self.vocab_size, p=self.matrix[current_idx])\n",
    "                yield self.id2token[chosen]\n",
    "                if chosen == self.token2id['<END>']:\n",
    "                    chosen = self.token2id['<START>']\n",
    "                current_idx = chosen\n",
    "        if self.n > 2:\n",
    "            current_idx = self.ngram2id[' '.join(['<START>'] * (self.n-1))]\n",
    "            for i in range(n):\n",
    "                if random_state:\n",
    "                    np.random.seed(random_state)\n",
    "                chosen = np.random.choice(self.vocab_size, p=self.matrix[current_idx])\n",
    "                yield self.id2token[chosen]\n",
    "                if chosen == self.token2id['<END>']:\n",
    "                    current_idx = self.ngram2id[' '.join(['<START>'] * (self.n-1))]\n",
    "                else:\n",
    "                    current_idx = self.ngram2id[' '.join(self.id2ngram[current_idx].split()[1:] +\n",
    "                                                         [self.id2token[chosen]])]\n",
    "            \n",
    "    @staticmethod\n",
    "    def tokenize(sentence: str) -> map:\n",
    "        return map(lambda x: x.strip(punctuation).replace('ё', 'е'),\n",
    "                   sentence.strip().lower().split())\n",
    "    \n",
    "    @staticmethod\n",
    "    def ngrammer(sentence: List[str], n: int) -> Generator[str, None, None]:\n",
    "        for i in range(len(sentence)-n+1):\n",
    "            yield ' '.join(sentence[i: i+n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Биграммная языковая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolstoy2 = NgramLanguageModel('anna_karenina.txt', 2)\n",
    "dostoevsky2 = NgramLanguageModel('besy_dostoevsky.txt', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тяга была так сказывает ото ржей не найдете ли \n",
      " кити не будет всегда чувствую как он понимает говорила она мужу с удобством без отдыха и ждали \n",
      " говорят о том положении человека та вода \n",
      " я покупаю сама княгиня тверская до сих пор пока уйдет с присоединившимся к нему точно так как не должна была счастлива буду избегать ее речь дорожа его жены чтобы не молчать и которой упоминалось о том как это со свернутыми набок голову при величайшем напряжении сил чувствовала всю эту осоку \n",
      " нет я завидую \n",
      " и рада что жена в мир для добра который\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([x for x in tolstoy2.generate_random_text()]).replace('<END>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "да марья тимофеевна в доме и от одной радости голосом знаешь ли \n",
      " еще в феврале когда девочке было ей расписывать про себя к публике начал возиться с патетическою иронией заметил липутин вскочил с самого святого фр \n",
      " когда бедная заезжая дама да зачем \n",
      " наши спины и так как же окруженною выбрала направление и к черту этих господ \n",
      " у господина средних лет \n",
      " кого ты ставрогину только то что… за сумасшедшего во главе юлию михайловну \n",
      " в грустном настроении духа \n",
      " в буфет которым он тогда вымирают народы и до свидания в него вынул из темной каморки еще\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([x for x in dostoevsky2.generate_random_text()]).replace('<END>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Триграммная языковая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolstoy3 = NgramLanguageModel('anna_karenina.txt', 3)\n",
    "dostoevsky3 = NgramLanguageModel('besy_dostoevsky.txt', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "зачем я здесь стоя в холодке вновь покрытой риги с необсыпавшимся еще пахучим листом лещинового решетника прижатого к облупленным свежим осиновым слегам соломенной крыши левин глядел то сквозь открытые ворота в которых ныне находятся инородцы и в-четвертых наконец чтобы было столько неудач и столько враждебных отношений между им и смеется \n",
      " он был здесь в москве в первый раз в сенях заинтересовался им \n",
      " это несчастие совершившимся фактом и стараюсь помочь и ей не во мне много хорошего \n",
      " они не думают о женихе и заняты и лишь он один теперь с варенькой… кажется что-то есть… может быть скучно на\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([x for x in tolstoy3.generate_random_text()]).replace('<END>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я положительно знаю что мне наскучила жизнь и за три дай просмотреть а то пожалуй и не сумею умереть кстати сегодня же я буду жить \n",
      " пошли очень скоро шептать но я только ищу причины почему люди не любили народа не страдали за него более чем приносили пользы их изгоняют или казнят \n",
      " тогда я начинал его ненавидеть… равно как и прежде хотел выйти однажды из службы давно уже на деле вникнуть лично и всегда… будем надеяться что и хмель соскочил \n",
      " не беспокойтесь я сама пойду что бы то ни было но отставил чашку на стол \n",
      " au diable\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([x for x in dostoevsky3.generate_random_text()]).replace('<END>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказания триграммной языковой чуть больше похожи на связный текст. Возможно, если мы и дальше будем увеличивать длину энграмм, качество порождаемого текста будет расти, однако всё острее будет стоять проблема количества обучающих данных. Чем более длинный контекст мы хотим учитывать, тем больше данных для обучения нам нужно. Кроме того, по-хорошему, подсчитываемые корпусные вероятности надо бы сглаживать, например, функцией softmax (на этапе случайного выбора следующего токена), чтобы порождаемый текст был меньше похож на склеенные кусочки предложений оригинала, однако это создаёт новую проблему: получающиеся таким образом энграммы отсутствуют в корпусе. Методы борьбы с этой проблемой находятся вне скоупа этого домашнего задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
