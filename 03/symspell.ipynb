{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from string import punctuation\n",
    "from nltk import sent_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict, Union, Generator\n",
    "\n",
    "punctuation += '«»–—…“”'\n",
    "punct = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1: str, sent_2: str) -> List[Tuple[str]]:\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> List[str]:\n",
    "    normalized_text = [(word.strip(punctuation)) for word in text.lower().split()]\n",
    "    normalized_text = [word for word in normalized_text if word]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a bigger corpus.\n",
    "\n",
    "# with open('corpus_10000.txt', 'w') as corpus, gzip.open('lenta-ru-news.csv.gz', 'rt') as archive:\n",
    "#     reader = csv.reader(archive, delimiter=',', quotechar='\"')\n",
    "#     for i, line in enumerate(reader):\n",
    "#         if i < 10000: # увеличьте количество текстов тут\n",
    "#             corpus.write(line[2].replace('\\xa0', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in open('corpus_10000.txt').read().splitlines():\n",
    "    sentences = sent_tokenize(text)\n",
    "    norm_sentences = [normalize(sentence) for sentence in sentences]\n",
    "    corpus += norm_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = Counter()\n",
    "for sentence in corpus:\n",
    "    WORDS.update(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(WORDS.values())\n",
    "def P(word): \n",
    "    return WORDS[word] / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspells = defaultdict(set)\n",
    "\n",
    "for word in WORDS:\n",
    "    for i in range(len(word)):\n",
    "        misspells[word[:i] + word[i+1:]].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('вице-пемьер', {'вице-премьер'}),\n",
       " ('вице-прмьер', {'вице-премьер'}),\n",
       " ('вице-преьер', {'вице-премьер'}),\n",
       " ('вице-премер', {'вице-премьер'}),\n",
       " ('вице-премьр', {'вице-премьер'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(misspells.items())[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correction(word: str) -> Union[str, None]:\n",
    "    best_option = (word, 0)\n",
    "    for i in range(len(word)):\n",
    "        candidate = misspells.get(word[:i] + word[i+1:])\n",
    "        if candidate is not None:\n",
    "            candidate, p = sorted(zip(candidate, (P(x) for x in candidate)), key=lambda x: x[1], reverse=True)[0]\n",
    "            if p > best_option[1]:\n",
    "                best_option = (candidate, p)\n",
    "    if best_option[1] > 0:\n",
    "        return best_option[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47 µs, sys: 0 ns, total: 47 µs\n",
      "Wall time: 53.2 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'солнце'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "get_correction('сомнце')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No language model (unigram probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for (true_word, bad_word) in word_pairs:\n",
    "        if bad_word in WORDS:\n",
    "            pred = bad_word\n",
    "        else:\n",
    "            pred = get_correction(bad_word)\n",
    "            if pred is None:\n",
    "                pred = bad_word\n",
    "        if pred == true_word:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if true_word == bad_word:\n",
    "            total_correct += 1\n",
    "            if true_word != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if true_word == pred:\n",
    "                mistaken_fixed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "0.21\n",
      "0.054\n"
     ]
    }
   ],
   "source": [
    "print(np.round(correct/total, 3))\n",
    "print(np.round(mistaken_fixed/total_mistaken, 3))\n",
    "print(np.round(correct_broken/total_correct, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a language model to range the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    def __init__(self, path: str, n: int = 3) -> None:\n",
    "        if n < 2:\n",
    "            raise Exception('Parameter N of an n-gram language model cannot be less than 2!')\n",
    "        self.n = n\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            self.text = f.read()\n",
    "        self.sentences = map(lambda sentence: ['<START>'] * (self.n - 1) + \\\n",
    "                             [x for x in self.tokenize(sentence) if x] + ['<END>'],\n",
    "                             sent_tokenize(self.text))\n",
    "        self.token2id = {}\n",
    "        if self.n > 2:\n",
    "            self.ngram2id = {}\n",
    "        self.ngrams = [Counter(), Counter()]\n",
    "        self.get_ngram_counts()\n",
    "        self.vocab_size = len(self.token2id)\n",
    "        self.id2token = {value: key for key, value in self.token2id.items()}\n",
    "        self.matrix = np.zeros((len(self.ngrams[0]), self.vocab_size))\n",
    "        if self.n > 2:\n",
    "            self.id2ngram = {value: key for key, value in self.ngram2id.items()}\n",
    "        self.populate_matrix()\n",
    "    \n",
    "    def get_ngram_counts(self) -> None:\n",
    "        if self.n == 2:\n",
    "            for sentence in self.sentences:\n",
    "                self.ngrams[0].update(sentence)\n",
    "                self.ngrams[1].update(list(self.ngrammer(sentence, 2)))\n",
    "                self.update_token2id(sentence)\n",
    "        if self.n > 2:\n",
    "            for sentence in self.sentences:\n",
    "                ngrams = list(self.ngrammer(sentence, self.n-1))\n",
    "                self.ngrams[0].update(ngrams)\n",
    "                self.ngrams[1].update(list(self.ngrammer(sentence, self.n)))\n",
    "                self.update_token2id(sentence)\n",
    "                self.update_ngram2id(ngrams)\n",
    "    \n",
    "    def update_token2id(self, tokens: List[str]) -> None:\n",
    "        for token in tokens:\n",
    "            if token not in self.token2id:\n",
    "                self.token2id[token] = len(self.token2id)\n",
    "    \n",
    "    def update_ngram2id(self, ngrams: List[str]) -> None:\n",
    "        for ngram in ngrams:\n",
    "            if ngram not in self.ngram2id:\n",
    "                self.ngram2id[ngram] = len(self.ngram2id)\n",
    "    \n",
    "    def populate_matrix(self):\n",
    "        if self.n == 2:\n",
    "            for ngram, count in self.ngrams[1].items():\n",
    "                src, dest = ngram.split()\n",
    "                self.matrix[self.token2id[src], self.token2id[dest]] = \\\n",
    "                    count / self.ngrams[0][src]\n",
    "        if self.n > 2:\n",
    "            for ngram, count in self.ngrams[1].items():\n",
    "                ngram_splitted = ngram.split()\n",
    "                src = ' '.join(ngram_splitted[:-1])\n",
    "                dest = ngram_splitted[-1]\n",
    "                self.matrix[self.ngram2id[src], self.token2id[dest]] = \\\n",
    "                    count / self.ngrams[0][src]\n",
    "    \n",
    "    def get_trigram_probability(self, word: str, prev_bigram: str) -> float:\n",
    "        if prev_bigram in self.ngram2id and word in self.token2id:\n",
    "            return self.matrix[self.ngram2id[prev_bigram], self.token2id[word]]\n",
    "        return P(word)\n",
    "            \n",
    "    @staticmethod\n",
    "    def tokenize(sentence: str) -> map:\n",
    "        return map(lambda x: x.strip(punctuation).replace('ё', 'е'),\n",
    "                   sentence.strip().lower().split())\n",
    "    \n",
    "    @staticmethod\n",
    "    def ngrammer(sentence: List[str], n: int) -> Generator[str, None, None]:\n",
    "        for i in range(len(sentence)-n+1):\n",
    "            yield ' '.join(sentence[i: i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_lm = NgramLanguageModel('corpus_10000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lm_correction(word: str, bigram: str) -> Union[str, None]:\n",
    "    best_option = (word, 0)\n",
    "    for i in range(len(word)):\n",
    "        candidate = misspells.get(word[:i] + word[i+1:])\n",
    "        if candidate is not None:\n",
    "            candidate, p = sorted(zip(candidate, (trigram_lm.get_trigram_probability(x, bigram) for x in candidate)),\n",
    "                                  key=lambda x: x[1], reverse=True)[0]\n",
    "            if p > best_option[1]:\n",
    "                best_option = (candidate, p)\n",
    "    if best_option[1] > 0:\n",
    "        return best_option[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 664 µs, sys: 8.33 ms, total: 8.99 ms\n",
      "Wall time: 58.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'солнце'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "get_lm_correction('сомнце', '<START> взошло')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = [('<START>, <START>')] * 2 + align_words(true[i], bad[i])\n",
    "    for i, (true_word, bad_word) in enumerate(word_pairs[2:]):\n",
    "        if bad_word in WORDS:\n",
    "            pred = bad_word\n",
    "        else:\n",
    "            pred = get_lm_correction(bad_word, word_pairs[i][1] + ' ' + word_pairs[i+1][1])\n",
    "            if pred is None:\n",
    "                pred = bad_word\n",
    "        if pred == true_word:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if true_word == bad_word:\n",
    "            total_correct += 1\n",
    "            if true_word != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if true_word == pred:\n",
    "                mistaken_fixed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.858\n",
      "0.181\n",
      "0.041\n"
     ]
    }
   ],
   "source": [
    "print(np.round(correct/total, 3))\n",
    "print(np.round(mistaken_fixed/total_mistaken, 3))\n",
    "print(np.round(correct_broken/total_correct, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might look like using a language model to choose the most probable result led to an improvement. But:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for (true_word, bad_word) in word_pairs:\n",
    "        pred = bad_word\n",
    "        if pred == true_word:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if true_word == bad_word:\n",
    "            total_correct += 1\n",
    "            if true_word != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if true_word == pred:\n",
    "                mistaken_fixed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.round(correct/total, 3))\n",
    "print(np.round(mistaken_fixed/total_mistaken, 3))\n",
    "print(np.round(correct_broken/total_correct, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we let the misspells be, the first metric (the main one) will be higher. That's why SymSpell is a really dubious algorithm. Let's give it its last chance — edit distance of 2 (we will be deleting two random characters in a word).\n",
    "\n",
    "P. S. The following cells are executed really slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspells = defaultdict(set)\n",
    "\n",
    "for word in WORDS:\n",
    "    for i in range(len(word)):\n",
    "        subword = word[:i] + word[i+1:]\n",
    "        for j in range(len(subword)):\n",
    "            misspells[subword[:j] + subword[j+1:]].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'солнца', 'солнце', 'солнцу'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspells['сонц']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lm_correction_ld2(word: str, bigram: str) -> Union[str, None]:\n",
    "    best_option = (word, 0)\n",
    "    for i in range(len(word)):\n",
    "        subword = word[:i] + word[i+1:]\n",
    "        for j in range(len(subword)):\n",
    "            candidate = misspells.get(subword[:j] + subword[j+1:])\n",
    "            if candidate is not None:\n",
    "                candidate, p = sorted(zip(candidate, (trigram_lm.get_trigram_probability(x, bigram) for x in candidate)),\n",
    "                                      key=lambda x: x[1], reverse=True)[0]\n",
    "                if p > best_option[1]:\n",
    "                    best_option = (candidate, p)\n",
    "    if best_option[1] > 0:\n",
    "        return best_option[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = [('<START>, <START>')] * 2 + align_words(true[i], bad[i])\n",
    "    for i, (true_word, bad_word) in enumerate(word_pairs[2:]):\n",
    "        if bad_word in WORDS:\n",
    "            pred = bad_word\n",
    "        else:\n",
    "            pred = get_lm_correction_ld2(bad_word, word_pairs[i][1] + ' ' + word_pairs[i+1][1])\n",
    "            if pred is None:\n",
    "                pred = bad_word\n",
    "        if pred == true_word:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if true_word == bad_word:\n",
    "            total_correct += 1\n",
    "            if true_word != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if true_word == pred:\n",
    "                mistaken_fixed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n",
      "0.094\n",
      "0.071\n"
     ]
    }
   ],
   "source": [
    "print(np.round(correct/total, 3))\n",
    "print(np.round(mistaken_fixed/total_mistaken, 3))\n",
    "print(np.round(correct_broken/total_correct, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only became worse, so it's safe to say that SymSpell doesn't work too well, at least for our synthetically generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
