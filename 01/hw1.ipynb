{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "построенный\n",
      "построенный\n",
      "None\n",
      "построен\n",
      "году\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import regex\n",
    "\n",
    "\"\"\"\n",
    "В regex поддерживаются частичные совпадения.\n",
    "При использовании флага partial=True ищется соответствие префикса строки левой части регулярного выражения.\n",
    "\"\"\"\n",
    "\n",
    "re_pattern = re.compile(r'\\w+енный')\n",
    "regex_pattern = regex.compile(r'\\w+енный')\n",
    "\n",
    "print(re.search(re_pattern, 'построенный').group(0))\n",
    "print(regex.search(regex_pattern, 'построенный').group(0))\n",
    "\n",
    "print(re.search(re_pattern, 'построен'))\n",
    "print(regex.search(regex_pattern, 'построен', partial=True).group(0))\n",
    "\n",
    "# Здесь работает не так, как хотелось бы.\n",
    "print(regex.search(regex_pattern, 'построен в 1921 году', partial=True).group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gramota.txt', 'r') as f:\n",
    "    text = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['обособление',\n",
       " 'обстоятельств',\n",
       " 'обстоятельство',\n",
       " 'второстепенный',\n",
       " 'член',\n",
       " 'предложения',\n",
       " 'который',\n",
       " 'обозначает',\n",
       " 'признак',\n",
       " 'действия',\n",
       " 'или',\n",
       " 'другого',\n",
       " 'признака',\n",
       " 'обстоятельства',\n",
       " 'поясняют',\n",
       " 'сказуемые',\n",
       " 'или',\n",
       " 'другие',\n",
       " 'члены',\n",
       " 'предложения']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "text_tokenized = [x.lower() for x in tokenize(text)]\n",
    "assert(all(not any(punct in token for punct in punctuation)\n",
    "           for token in text_tokenized))\n",
    "text_tokenized[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')\n",
    "stems = {token: stemmer.stem(token) for token in text_tokenized}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'обстоятельств': ['обстоятельств', 'обстоятельство', 'обстоятельства'],\n",
       " 'член': ['член', 'члены'],\n",
       " 'предложен': ['предложения', 'предложениях', 'предложении'],\n",
       " 'котор': ['который', 'которые'],\n",
       " 'признак': ['признак', 'признака'],\n",
       " 'друг': ['другого', 'другие'],\n",
       " 'сказуем': ['сказуемые', 'сказуемым'],\n",
       " 'выделя': ['выделять', 'выделяется', 'выделяются'],\n",
       " 'случа': ['случаях', 'случай', 'случаи'],\n",
       " 'рассмотр': ['рассмотрим', 'рассмотрите'],\n",
       " 'перв': ['первый', 'первом'],\n",
       " 'выраж': ['выражены', 'выражено'],\n",
       " 'нареч': ['наречием', 'наречия'],\n",
       " 'деепричаст': ['деепричастием', 'деепричастий'],\n",
       " 'оборот': ['оборотом', 'оборотами', 'оборот'],\n",
       " 'улиц': ['улицу', 'улице', 'улицах'],\n",
       " 'существительн': ['существительным', 'существительными'],\n",
       " 'эт': ['этого', 'это'],\n",
       " 'выражен': ['выражением', 'выраженные'],\n",
       " 'он': ['он', 'оно', 'они', 'она'],\n",
       " 'так': ['так', 'такие', 'такими'],\n",
       " 'как': ['как', 'каких', 'какой', 'каким', 'каком'],\n",
       " 'втор': ['втором', 'второй'],\n",
       " 'не': ['не', 'нее'],\n",
       " 'обособля': ['обособляется', 'обособляются', 'обособляться'],\n",
       " 'предлог': ['предлогом', 'предлога', 'предлогами'],\n",
       " 'след': ['следующие', 'следует'],\n",
       " 'отвеча': ['отвечающие', 'отвечают'],\n",
       " 'вопрос': ['вопросы', 'вопрос'],\n",
       " 'причин': ['причины', 'причине'],\n",
       " 'цел': ['цели', 'целью', 'целый'],\n",
       " 'образ': ['образа', 'образом'],\n",
       " 'услов': ['условия', 'условии'],\n",
       " 'был': ['было', 'были'],\n",
       " 'вид': ['видов', 'вид'],\n",
       " 'начина': ['начинается', 'начинаются'],\n",
       " 'сравнительн': ['сравнительными', 'сравнительный'],\n",
       " 'сто': ['стоя', 'стоят'],\n",
       " 'слов': ['слов', 'словами']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "stems2words = defaultdict(list)\n",
    "for key, value in stems.items():\n",
    "    stems2words[value].append(key)\n",
    "{key: value for key, value in stems2words.items() if len(value) > 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К одинаковой основе были приведены разные слова: \"выражением\" и \"выраженные\", \"не\" и \"нее\", \"следующие\" и \"следует\" (хотя исторически это деепричастие, то есть форма этого глагола), \"цели\"/\"целью\" и \"целый\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "он\n",
      "не\n"
     ]
    }
   ],
   "source": [
    "# Стеммер не справляется с супплетивными парадигмами, поскольку корни, а значит, и префиксы, разные.\n",
    "\n",
    "print(stems['она'])\n",
    "print(stems['нее'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'обстоятельств': 'обстоятельств',\n",
       " 'признак': 'признак',\n",
       " 'могут': 'могут',\n",
       " 'например': 'например',\n",
       " 'дворник': 'дворник',\n",
       " 'встает': 'встает',\n",
       " 'может': 'может',\n",
       " 'подряд': 'подряд',\n",
       " 'сидел': 'сидел',\n",
       " 'город': 'город',\n",
       " 'вопрос': 'вопрос',\n",
       " 'мороз': 'мороз',\n",
       " 'оборот': 'оборот',\n",
       " 'вдруг': 'вдруг',\n",
       " 'имеет': 'имеет',\n",
       " 'перед': 'перед'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{key: value for key, value in stems.items() if len(key) > 4 and key == value}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае словоформ \"могут\" и \"может\", \"встает\", \"сидел\", \"имеет\" — налицо ошибка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Местоимения — частотная и слабо нагруженная семантически часть речи.\n",
    "\n",
    "'почему' in stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# То же самое в ещё большей степени относится к предлогам, даже к их усечённым вариантам.\n",
    "\n",
    "'пред' in stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Здесь уже зависит от корпуса и контекста, но, например, в контексте\n",
    "# гуманитарного или лингвистического дискурса слово \"слово\" — это стоп-слово.\n",
    "\n",
    "'слово' in stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разные формы одних и тех же слов включены в этот список, но почему это не сделано последовательно?\n",
    "\n",
    "assert 'один' in stops\n",
    "'одна' in stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"то\" и \"нибудь\" есть в списке, но почему забыли \"либо\"?\n",
    "\n",
    "assert 'то' in stops and 'нибудь' in stops\n",
    "'либо' in stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('zhukov.txt', 'r') as f:\n",
    "    text = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK + regex + pymorphy2[fast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«Новая газета» опубликовала сокращенный вариант заключения экспертов Александра Коршикова и Анны Осокиной, которые анализировали видео из блога студента Высшей школы экономики Егора Жукова, обвиняемого в призывах к экстремизму в интернете (часть 2 статьи 280 УК).',\n",
       " 'Эксперты нашли экстремизм в четырех видео на YouTube-канале Жукова.',\n",
       " 'По их мнению, в видео «отчетливо прослеживается мотив политической ненависти или вражды к действующей власти».',\n",
       " 'В заключении говорится, что Жуков приписывает действующей власти «враждебные действия», называет ее врагами, источником зла и вреда.',\n",
       " 'Кроме того, студент призывает с этой властью бороться.',\n",
       " '«Автор описывает катастрофическое положение населения в России, как следствие целенаправленной политики действующей власти <...>.',\n",
       " 'Основным идеалом, которого следует достичь, является \"смена власти в России\" для того, чтобы \"Россия стала свободной\"», — цитирует издание экспертов.',\n",
       " 'В одном из видео, в котором Жуков размышлял об итогах митинга 7 октября 2017 года, эксперты увидели призыв бороться любым способом, «что включает в себя и действия насильственного характера, в частности, насильственный захват власти, вооруженный мятеж».',\n",
       " 'Егор Жуков — фигурант «московского дела».',\n",
       " 'Сначала его обвиняли в участии в массовых беспорядках (часть 2 статьи 212 УК).',\n",
       " 'В суде говорили, что среди материалов есть видео, где Жуков показывает кому-то направо на акции 27 июля.',\n",
       " 'Из-за этого студента 2 августа отправили в СИЗО.',\n",
       " 'Однако через месяц дело о массовых беспорядках против Жукова закрыли, ему предъявили новое обвинение в призывах к экстремизму.',\n",
       " 'По ходатайстве следствие суд перевел студента под домашний арест.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение на предложения безошибочно, придраться негде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«',\n",
       " 'Новая',\n",
       " 'газета',\n",
       " '»',\n",
       " 'опубликовала',\n",
       " 'сокращенный',\n",
       " 'вариант',\n",
       " 'заключения',\n",
       " 'экспертов',\n",
       " 'Александра',\n",
       " 'Коршикова',\n",
       " 'и',\n",
       " 'Анны',\n",
       " 'Осокиной',\n",
       " ',',\n",
       " 'которые',\n",
       " 'анализировали',\n",
       " 'видео',\n",
       " 'из',\n",
       " 'блога',\n",
       " 'студента',\n",
       " 'Высшей',\n",
       " 'школы',\n",
       " 'экономики',\n",
       " 'Егора',\n",
       " 'Жукова',\n",
       " ',',\n",
       " 'обвиняемого',\n",
       " 'в',\n",
       " 'призывах',\n",
       " 'к',\n",
       " 'экстремизму',\n",
       " 'в',\n",
       " 'интернете',\n",
       " '(',\n",
       " 'часть',\n",
       " '2',\n",
       " 'статьи',\n",
       " '280',\n",
       " 'УК',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# возьмём регулярку из wordpunct_tokenize и слегка дополним:\n",
    "# вся пунктуация будет отдельным токеном, кроме композитного символа \"<...>\"\n",
    "tokenize = RegexpTokenizer(r'\\w+|<...>|[«»\\'\",.:;!?\\(\\)-–—]|[^\\w\\s]+').tokenize\n",
    "tokens = [tokenize(x) for x in sentences]\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.morph = MorphAnalyzer()\n",
    "        self.cache = {}\n",
    "    \n",
    "    def lemmatize(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        norm = self.morph.parse(token)[0].normal_form\n",
    "        self.cache[token] = norm\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['«',\n",
       " 'новый',\n",
       " 'газета',\n",
       " '»',\n",
       " 'опубликовать',\n",
       " 'сократить',\n",
       " 'вариант',\n",
       " 'заключение',\n",
       " 'эксперт',\n",
       " 'александр',\n",
       " 'коршикова',\n",
       " 'и',\n",
       " 'анна',\n",
       " 'осокин',\n",
       " ',',\n",
       " 'который',\n",
       " 'анализировать',\n",
       " 'видео',\n",
       " 'из',\n",
       " 'блог',\n",
       " 'студент',\n",
       " 'высокий',\n",
       " 'школа',\n",
       " 'экономика',\n",
       " 'егор',\n",
       " 'жуков',\n",
       " ',',\n",
       " 'обвинять',\n",
       " 'в',\n",
       " 'призыв',\n",
       " 'к',\n",
       " 'экстремизм',\n",
       " 'в',\n",
       " 'интернет',\n",
       " '(',\n",
       " 'часть',\n",
       " '2',\n",
       " 'статья',\n",
       " '280',\n",
       " 'ук',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize = Lemmatizer().lemmatize\n",
    "lemmas = [[lemmatize(token) for token in sentence] for sentence in tokens]\n",
    "lemmas[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['новый',\n",
       " 'газета',\n",
       " 'опубликовывать',\n",
       " 'сокращать',\n",
       " 'вариант',\n",
       " 'заключение',\n",
       " 'эксперт',\n",
       " 'александр',\n",
       " 'коршиков?',\n",
       " 'и',\n",
       " 'анна',\n",
       " 'осокина',\n",
       " 'который',\n",
       " 'анализировать',\n",
       " 'видео',\n",
       " 'из',\n",
       " 'блог',\n",
       " 'студент',\n",
       " 'высокий',\n",
       " 'школа',\n",
       " 'экономика',\n",
       " 'егор',\n",
       " 'жуков',\n",
       " 'обвиняемый',\n",
       " 'в',\n",
       " 'призыв',\n",
       " 'к',\n",
       " 'экстремизм',\n",
       " 'в',\n",
       " 'интернет',\n",
       " 'часть',\n",
       " 'статья',\n",
       " 'ук',\n",
       " 'эксперт',\n",
       " 'находить',\n",
       " 'экстремизм',\n",
       " 'в',\n",
       " 'четыре',\n",
       " 'видео',\n",
       " 'на',\n",
       " 'YouTube??',\n",
       " 'канал',\n",
       " 'жуков']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "def sent2tokens(sentence: str) -> List[str]:\n",
    "    return re.findall(r'(?<={).*?(?=})', sentence, re.DOTALL)\n",
    "\n",
    "# файл получен командой ./mystem -c -l -s -d <INPUT_PATH> <OUTPUT_PATH>\n",
    "with open('zhukov_lemmas.txt', 'r') as f:\n",
    "    lemmas = [sent2tokens(x) for x in f.read().strip().split('{\\s}')]\n",
    "lemmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 8\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. myStem выделяет меньше предложений, чем NLTK, сливая многие в одно. __1:0__\n",
    "2. Регулярка токенизирует ровно так, как нам нужно, а её результат легко почистить (убрать токены-пунктуацию). myStem пропускает всю пунктуацию, не заключая её в фигурные скобки, поэтому, если она нам всё-таки нужна, придётся написать регулярку, чтобы извлекать её из выдачи myStem. Кроме того, чтобы распарсить его выход, уже пришлось написать одну регулярку. Единственный минус текущей регулярки на этом тексте — она разбивает на отдельные слова токены с дефисом (\"кто-то\"), но это легко поправить ещё одной группой через вертикальную черту. __2:0__\n",
    "3. Качество лемматизации примерно одинаковое, хотя и здесь myStem слегка проигрывает, что довольно неожиданно, так как в нём реализовано контекстное снятие омонимии. Однако оно работает с ошибками, приводя глаголы к форме не того вида (\"находить\" вместо \"найти\" и пр.). Для словоформ \"ее\" и \"того\" приводятся неверные начальные формы (\"ее\" и \"то\" соответственно). Всех этих ошибок pymorphy2 не делает. __3:0__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
